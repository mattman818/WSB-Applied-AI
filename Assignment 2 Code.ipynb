{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Bi_pEl91J4q1"},"outputs":[],"source":["# Install required visualization library\n","!pip install matplotlib\n","import matplotlib.pyplot as plt\n","from collections import defaultdict\n","\n","# Import PyTorch and its neural network modules\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import time\n","\n","# Check if GPU is available, otherwise use CPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","\n","# Purpose of this code:\n","# 1. Experiment with each of the parameters that\n","#    affect how the model is trained\n","# 2. See if there is a visual that can be used to better\n","#    show what the effect on the model it has\n","# 3. See & explain the effect on training the model\n","\n","# Model hyperparameters\n","batch_size = 128        # Number of samples processed together\n","block_size = 64         # Size of the context window\n","max_iters = 300        # Maximum number of training iterations\n","eval_interval = 100    # How often to evaluate the model\n","learning_rate = 1e-3   # Step size for optimization\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU/CPU setting\n","eval_iters = 200      # Number of iterations for evaluation\n","n_embd = 256          # Size of embedding vectors\n","n_head = 4            # Number of attention heads\n","n_layer = 6           # Number of transformer layers\n","dropout = 0.0         # Dropout rate for regularization\n","smoothing_factor = 0.95  # Factor for smoothing metrics\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(1337)\n","\n","# Download the training text (Shakespeare's works)\n","!wget http://www.gutenberg.org/cache/epub/100/pg100.txt\n","\n","# Read the text file\n","with open('pg100.txt', 'r', encoding='utf-8') as f1:\n","    text = f1.read()\n","\n","# Create vocabulary from unique characters in text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","\n","# Create mapping dictionaries\n","stoi = {ch: i for i, ch in enumerate(chars)}    # Character to integer mapping\n","itos = {i: ch for i, ch in enumerate(chars)}    # Integer to character mapping\n","\n","# Define encoding and decoding functions\n","encode = lambda s: [stoi[c] for c in s]         # Convert text to integers\n","decode = lambda l: ''.join([itos[i] for i in l])  # Convert integers back to text\n","\n","# Split data into training and validation sets\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9 * len(data))                        # Use 90% for training\n","train_data = data[:n]                           # Training set\n","val_data = data[n:]                             # Validation set"]},{"cell_type":"code","source":["# Data loading function for training and validation\n","def get_batch(split):\n","    # Generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))  # Random starting indices\n","    x = torch.stack([data[i:i+block_size] for i in ix])        # Input sequences\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])    # Target sequences (shifted by 1)\n","    x, y = x.to(device), y.to(device)                          # Move to GPU if available\n","    return x, y\n","\n","# Decorator to disable gradient calculation for evaluation\n","@torch.no_grad()\n","def estimate_loss(model, split):\n","    # Calculate average loss over multiple batches\n","    out = {}\n","    model.eval()  # Set model to evaluation mode\n","    losses = torch.zeros(eval_iters)\n","    for k in range(eval_iters):\n","        X, Y = get_batch(split)\n","        logits, loss = model(X, Y)\n","        losses[k] = loss.item()\n","    model.train()  # Set model back to training mode\n","    return losses.mean()\n","\n","class Head(nn.Module):\n","    \"\"\" Single head of self-attention \"\"\"\n","    def __init__(self, head_size):\n","        super().__init__()\n","        # Linear transformations for key, query, and value\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        # Register attention mask\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)    # Generate keys\n","        q = self.query(x)  # Generate queries\n","        # Compute attention scores\n","        wei = q @ k.transpose(-2,-1) * C**-0.5  # Scaled dot-product attention\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Apply causal mask\n","        wei = F.softmax(wei, dim=-1)  # Convert to probabilities\n","        # Weighted aggregation of values\n","        v = self.value(x)\n","        out = wei @ v\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)  # Final projection layer\n","\n","    def forward(self, x):\n","        # Concatenate outputs from all heads\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.proj(out)  # Project back to original dimension\n","        return out\n","\n","class FeedForward(nn.Module):\n","    \"\"\" Simple feed-forward network \"\"\"\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),  # Expand dimension\n","            nn.ReLU(),                       # Non-linearity\n","            nn.Linear(4 * n_embd, n_embd),  # Project back\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block combining attention and computation \"\"\"\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)  # Self-attention layer\n","        self.ffwd = FeedForward(n_embd)                   # Feed-forward layer\n","\n","    def forward(self, x):\n","        x = x + self.sa(x)      # Attention with residual connection\n","        x = x + self.ffwd(x)    # Feed-forward with residual connection\n","        return x\n","\n","class BigramLanguageModel(nn.Module):\n","    \"\"\" Main language model \"\"\"\n","    def __init__(self,n_head):\n","        super().__init__()\n","        # Embedding layers\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        # Transformer blocks\n","        self.blocks = nn.Sequential(\n","            Block(n_embd, n_head=n_head),\n","            Block(n_embd, n_head=n_head),\n","            Block(n_embd, n_head=n_head),\n","        )\n","        self.lm_head = nn.Linear(n_embd, vocab_size)  # Final layer for token prediction\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","        # Get token and position embeddings\n","        tok_emb = self.token_embedding_table(idx)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n","        x = tok_emb + pos_emb  # Combine embeddings\n","        x = self.blocks(x)     # Pass through transformer blocks\n","        logits = self.lm_head(x)  # Get predictions\n","\n","        # Calculate loss if targets provided\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # Generate new tokens one at a time\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]  # Crop to last block_size tokens\n","            logits, _ = self(idx_cond)       # Get predictions\n","            logits = logits[:, -1, :]        # Focus on last token\n","            probs = F.softmax(logits, dim=-1)  # Get probabilities\n","            idx_next = torch.multinomial(probs, num_samples=1)  # Sample next token\n","            idx = torch.cat((idx, idx_next), dim=1)  # Append to sequence\n","        return idx\n","\n","class MetricsTracker:\n","    \"\"\"Class for tracking and visualizing training metrics across different model runs\"\"\"\n","    def __init__(self):\n","        # Initialize dictionaries to store different types of losses for each run\n","        self.train_losses = {}\n","        self.val_losses = {}\n","        self.gradient_norms = {}\n","        self.current_run = 'default'  # Track current experiment name\n","\n","    def set_run(self, run_name):\n","        \"\"\"Initialize a new experimental run with given name\"\"\"\n","        self.current_run = run_name\n","        # Create empty lists for storing metrics for this run\n","        self.train_losses[run_name] = []\n","        self.val_losses[run_name] = []\n","        self.gradient_norms[run_name] = []\n","\n","    def update(self, *args, **kwargs):\n","        \"\"\"Update metrics either from dictionary or individual values\"\"\"\n","        if len(args) == 1 and isinstance(args[0], dict):\n","            # Handle dictionary input\n","            metrics_dict = args[0]\n","            if 'train_loss' in metrics_dict:\n","                self.train_losses[self.current_run].append(metrics_dict['train_loss'])\n","            if 'gradient_norm' in metrics_dict:\n","                self.gradient_norms[self.current_run].append(metrics_dict['gradient_norm'])\n","        elif len(args) == 3:\n","            # Handle individual inputs (iteration, train_loss, val_loss)\n","            iter, train_loss, val_loss = args\n","            self.train_losses[self.current_run].append(train_loss)\n","            self.val_losses[self.current_run].append(val_loss)\n","\n","    def plot_comparison(self, metric_type='train'):\n","        \"\"\"Plot comparison of losses across different runs\"\"\"\n","        plt.figure(figsize=(10, 6))\n","        for run_name in self.train_losses.keys():\n","            if metric_type == 'train':\n","                losses = self.train_losses[run_name]\n","                plt.plot(losses, label=f'{run_name} - Training Loss')\n","            elif metric_type == 'val':\n","                losses = self.val_losses[run_name]\n","                plt.plot(losses, label=f'{run_name} - Validation Loss')\n","        plt.title(f'{metric_type.capitalize()} Loss Comparison')\n","        plt.xlabel('Steps')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","\n","def compute_gradient_norm(model):\n","    \"\"\"Calculate the L2 norm of gradients for all model parameters\"\"\"\n","    total_norm = 0\n","    for p in model.parameters():\n","        if p.grad is not None:\n","            param_norm = p.grad.data.norm(2)\n","            total_norm += param_norm.item() ** 2\n","    total_norm = total_norm ** 0.5\n","    return total_norm\n","\n","def train_model(model, optimizer, max_iters, eval_interval, metrics_tracker):\n","    \"\"\"Main training loop for the model\"\"\"\n","    for iter in range(max_iters):\n","        # Evaluate model periodically\n","        if iter % eval_interval == 0:\n","            train_loss = estimate_loss(model, 'train')\n","            val_loss = estimate_loss(model, 'val')\n","            metrics_tracker.update(iter, train_loss, val_loss)\n","            #print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n","\n","        # Get batch of training data\n","        xb, yb = get_batch('train')\n","\n","        # Forward and backward passes\n","        logits, loss = model(xb, yb)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track training metrics\n","        metrics_tracker.update({\n","            'train_loss': loss.item(),\n","            'gradient_norm': compute_gradient_norm(model)\n","        })\n","\n","\n","# Initialize metrics tracker and run experiments with different dropout values\n","metrics_tracker = MetricsTracker()\n","\n","# Train model dropout = 0.1\n","#dropout = 0.1\n","model1 = BigramLanguageModel(n_head)\n","model1.to(device)\n","optimizer1 = torch.optim.AdamW(model1.parameters(), lr=learning_rate)\n","metrics_tracker.set_run('dropout = 0.1')\n","start_time = time.time()\n","train_model(model1, optimizer1, max_iters, eval_interval, metrics_tracker)\n","print(f\"Model 1 Training time: {time.time() - start_time:.2f} seconds\")\n","'''\n","#Train model dropout = 0.2\n","dropout = 0.2\n","model2 = BigramLanguageModel(n_head)\n","model2.to(device)\n","optimizer2 = torch.optim.AdamW(model2.parameters(), lr=learning_rate)\n","metrics_tracker.set_run('dropout = 0.2')\n","start_time = time.time()\n","train_model(model2, optimizer2, max_iters, eval_interval, metrics_tracker)\n","print(f\"Model 2 Training time: {time.time() - start_time:.2f} seconds\")\n","\n","#Train model dropout = 0.3\n","dropout = 0.3\n","model3 = BigramLanguageModel(n_head)\n","model3.to(device)\n","optimizer3 = torch.optim.AdamW(model3.parameters(), lr=learning_rate)\n","metrics_tracker.set_run('dropout = 0.3')\n","start_time = time.time()\n","train_model(model3, optimizer3, max_iters, eval_interval, metrics_tracker)\n","print(f\"Model 3 Training time: {time.time() - start_time:.2f} seconds\")\n","'''\n","# Visualize results\n","metrics_tracker.plot_comparison('train')\n","metrics_tracker.plot_comparison('val')\n"],"metadata":{"id":"rwiRuLGVJ-tS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate text from the model\n","#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","#generated = decode(model.generate(context, max_new_tokens=2000)[0].tolist())\n","#print(generated)"],"metadata":{"id":"2sc0m6OYKDnN"},"execution_count":null,"outputs":[]}]}
