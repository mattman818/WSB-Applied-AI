{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOozqJUaYmHfmbeRyHUZqNe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JO0AU2oUNNgA","executionInfo":{"status":"error","timestamp":1734532779075,"user_tz":360,"elapsed":288,"user":{"displayName":"Matt Seitz","userId":"11341306287493563336"}},"outputId":"3a4586d5-0df2-410f-980e-d47f716d49ae"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'streamlit'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8bcdcc12882b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m  \u001b[0;31m# For creating the web interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAI\u001b[0m  \u001b[0;31m# OpenAI's chat models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCharacterTextSplitter\u001b[0m  \u001b[0;31m# For splitting text into chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["#Badger Herald GPT\n","\n","# Import necessary libraries\n","import streamlit as st  # For creating the web interface\n","from langchain_openai import ChatOpenAI, OpenAI  # OpenAI's chat models\n","from langchain.text_splitter import CharacterTextSplitter  # For splitting text into chunks\n","from langchain_community.document_loaders import TextLoader  # For loading text documents\n","from langchain.memory import ConversationBufferMemory  # For maintaining conversation history\n","from langchain_community.vectorstores import FAISS  # For vector storage and similarity search\n","from langchain.chains import ConversationalRetrievalChain  # For creating conversation chains\n","from langchain_openai import OpenAIEmbeddings  # For creating text embeddings\n","\n","# Set up the web app title\n","st.title(\"The Badger GPT\")\n","\n","# Get the OpenAI API key from Streamlit secrets\n","openai_api_key = st.secrets[\"OPENAI_API_KEY\"]\n","\n","# Define the path to the default text file\n","DEFAULT_TEXT_PATH = \"default.txt\"\n","\n","# Initialize session state variables to persist data between reruns\n","if \"conversation_chain\" not in st.session_state:\n","    st.session_state.conversation_chain = None\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","if \"vectorstore\" not in st.session_state:\n","    # Process the text file and create embeddings only once when the app starts\n","    try:\n","        # Read the default text file\n","        with open(DEFAULT_TEXT_PATH, 'r') as file:\n","            text = file.read()\n","            # Configure text splitting parameters\n","            text_splitter = CharacterTextSplitter(\n","                separator=\"\\n\",\n","                chunk_size=500,  # Size of each text chunk\n","                chunk_overlap=100,  # Overlap between chunks to maintain context\n","                length_function=len\n","            )\n","            # Split the text into chunks\n","            chunks = text_splitter.split_text(text)\n","\n","            # Create embeddings for the text chunks\n","            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n","            st.session_state.vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)\n","    except FileNotFoundError:\n","        st.error(f\"Default file {DEFAULT_TEXT_PATH} not found!\")\n","        st.stop()\n","\n","def generate_response(query):\n","    # Initialize the conversation chain if it doesn't exist\n","    if st.session_state.conversation_chain is None:\n","        # Set up conversation memory\n","        memory = ConversationBufferMemory(\n","            memory_key='chat_history',\n","            return_messages=True\n","        )\n","\n","        # Initialize the language model\n","        llm = ChatOpenAI(\n","            model_name=\"gpt-3.5-turbo-16k\",\n","            temperature=0.7,  # Controls randomness in responses\n","            openai_api_key=openai_api_key\n","        )\n","\n","        # Set up the retriever with search parameters\n","        retriever = st.session_state.vectorstore.as_retriever(\n","            search_kwargs={\"k\": 3}  # Number of relevant chunks to retrieve\n","        )\n","\n","        # Create the conversation chain\n","        st.session_state.conversation_chain = ConversationalRetrievalChain.from_llm(\n","            llm=llm,\n","            retriever=retriever,\n","            memory=memory\n","        )\n","\n","    # Generate response using the conversation chain\n","    response = st.session_state.conversation_chain({\"question\": query})\n","    return response['answer']\n","\n","# Add sample questions to the sidebar\n","st.sidebar.header(\"Sample Questions\")\n","sample_questions = [\n","    \"How is the UW madison womens volleyball team doing?\",\n","    \"How is the badger basketball team doing?\",\n","    \"Where are some good bookstores on campus?\",\n","    \"Are there any art exhibitions on campus?\",\n","    \"Is there any theatre to watch on campus?\",\n","    \"Are the northern lights visible from UW madison?\"\n","]\n","\n","st.sidebar.write(\"Try asking these questions:\")\n","for question in sample_questions:\n","    if st.sidebar.button(question):\n","        # When a sample question is clicked, use it as input\n","        response = generate_response(question)\n","        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n","\n","# Create the chat interface using a form\n","with st.form(\"my_form\"):\n","    text = st.text_area('Enter your question:', '')\n","    submitted = st.form_submit_button(\"Submit\")\n","\n","    if submitted:\n","        # Generate response and update chat history\n","        response = generate_response(text)\n","        st.session_state.messages.append({\"role\": \"user\", \"content\": text})\n","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n","\n","# Display the chat history\n","for message in st.session_state.messages:\n","    if message[\"role\"] == \"user\":\n","        st.write(\"You:\", message[\"content\"])\n","    else:\n","        st.write(\"Assistant:\", message[\"content\"])\n","\n","# Add a button to clear the chat history\n","if st.button(\"Clear Chat\"):\n","    st.session_state.conversation_chain = None\n","    st.session_state.messages = []"]}]}